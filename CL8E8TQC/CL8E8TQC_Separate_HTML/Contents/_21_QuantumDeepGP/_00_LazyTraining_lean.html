<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Lean Notebook Viewer</title>
  <script>
    // MathJax config — from MATHJAX_CONFIG in renderer.js (single source of truth).
    MathJax = {"tex":{"inlineMath":[["$","$"]],"displayMath":[["$$","$$"]],"processEscapes":true},"options":{"skipHtmlTags":["script","noscript","style","textarea","pre","code"],"menuOptions":{"settings":{"enrich":false,"collapsible":false,"speech":false,"braille":false,"assistiveMml":false}}},"startup":{"typeset":false}};
  </script>
  <script async src="../../_libs/tex-svg.js"></script>
  <script src="../../_libs/marked.min.js"></script>
  <script src="../../_libs/mermaid.min.js"></script>
  <script src="../../_libs/viz-standalone.js" async></script>
  <style>
    /* Injected from style.css by htmlExporter.ts — do NOT edit here. Edit style.css instead. */
    /* style.css — LeanNotebook VSCode Extension
   Identical CSS for both the static HTML viewer and the Extension WebView.
   Overrides VSCode user themes to prioritize the Lean brand.
*/

@import url('https://fonts.googleapis.com/css2?family=Source+Serif+4:ital,wght@0,400;0,600;1,400&family=Fira+Code:wght@400;500&family=Inter:wght@300;400;500;600&display=swap');

:root {
  /* Lean brand: white + blue */
  --bg: #f4f7fb;
  --surface: #ffffff;
  --surface-alt: #eef2f8;
  --border: #d0daea;
  --border-soft: #e4eaf4;
  --text: #1a2233;
  --text-muted: #4a5a78;
  --text-dim: #8a9ab8;
  /* Lean blue palette */
  --blue: #2563eb;
  --blue-light: #3b82f6;
  --blue-pale: #dbeafe;
  --blue-dim: #93c5fd;
  --blue-dark: #1d4ed8;
  /* syntax */
  --hl-keyword: #1d4ed8;
  --hl-tactic: #7c3aed;
  --hl-type: #0369a1;
  --hl-string: #15803d;
  --hl-number: #b45309;
  --hl-comment: #94a3b8;
  --hl-op: #475569;
  /* code bg */
  --code-bg: #f0f4ff;
  --radius: 7px;
  --font-prose: 'Source Serif 4', Georgia, serif;
  --font-code: 'Fira Code', 'Courier New', monospace;
  --font-ui: 'Inter', system-ui, sans-serif;
  --shadow-sm: 0 1px 3px rgba(37, 99, 235, .08), 0 1px 2px rgba(0, 0, 0, .04);
  --shadow-md: 0 4px 12px rgba(37, 99, 235, .10), 0 1px 4px rgba(0, 0, 0, .05);
}

*,
*::before,
*::after {
  box-sizing: border-box;
  margin: 0;
  padding: 0;
}

html {
  scroll-behavior: smooth;
  width: 100%;
  height: 100%;
}

body {
  background: var(--bg);
  color: var(--text);
  font-family: var(--font-ui);
  font-size: 16px;
  line-height: 1.6;
  min-height: 100vh;
  width: 100%;
}

/* ================================================================
   Overall Layout: Sidebar + Main Content
   Extension WebView: #layout
   HTML Export: #app (topbar + sidebar + notebook)
   ================================================================ */

/* --- Common root grid (#app) --- */
#app {
  display: grid;
  grid-template-columns: 260px 1fr;
  grid-template-rows: auto 1fr;
  min-height: 100vh;
}

#topbar {
  grid-column: 1 / -1;
  background: var(--surface);
  border-bottom: 1px solid var(--border);
  padding: 0 28px;
  height: 56px;
  display: flex;
  align-items: center;
  gap: 14px;
  position: sticky;
  top: 0;
  z-index: 100;
  box-shadow: var(--shadow-sm);
}

#topbar .logo {
  font-family: var(--font-code);
  font-size: 14px;
  color: var(--blue);
  font-weight: 600;
  letter-spacing: .02em;
  white-space: nowrap;
}

#topbar .logo span {
  color: var(--text-dim);
  font-weight: 400;
}

#topbar .doc-title {
  font-size: 14px;
  font-weight: 400;
  color: var(--text-muted);
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

#topbar .sep {
  color: var(--border);
}

#view-toggle {
  margin-left: auto;
  display: flex;
  align-items: center;
  background: var(--surface-alt);
  border: 1px solid var(--border);
  border-radius: 99px;
  padding: 3px;
  gap: 2px;
}

#view-toggle label {
  font-family: var(--font-code);
  font-size: 11px;
  font-weight: 500;
  padding: 3px 12px;
  border-radius: 99px;
  cursor: pointer;
  color: var(--text-muted);
  transition: background .15s, color .15s;
  user-select: none;
  white-space: nowrap;
}

#view-toggle input[type=radio] {
  display: none;
}

#view-toggle input[type=radio]:checked+label {
  background: var(--blue);
  color: #fff;
  box-shadow: 0 1px 3px rgba(37, 99, 235, .25);
}

#notebook {
  padding: 48px 60px;
  overflow-y: auto;
  overflow-x: hidden;
}

#lean-raw {
  display: none;
  padding: 48px 60px;
  max-width: none;
  grid-column: 2;
  overflow: auto;
}

#lean-raw pre {
  background: var(--code-bg);
  border: 1px solid var(--border);
  border-radius: var(--radius);
  padding: 24px;
  font-family: var(--font-code);
  font-size: 13px;
  line-height: 1.7;
  tab-size: 2;
  overflow-x: auto;
  box-shadow: var(--shadow-sm);
  white-space: pre;
}

#app.lean-mode {
  grid-template-columns: 0 1fr;
}

#app.lean-mode #sidebar {
  display: none;
}

#app.lean-mode #lean-raw {
  grid-column: 1 / -1;
  padding: 48px 60px;
  display: block;
}

@media(max-width:900px) {
  #app {
    grid-template-columns: 1fr;
  }

  #app #sidebar {
    display: none;
  }

  #notebook {
    padding: 28px 20px;
  }

  #lean-raw {
    grid-column: 1;
    padding: 28px 20px;
  }
}

/* ---- Sidebar ---- */
#sidebar {
  background: var(--surface);
  border-right: 1px solid var(--border);
  overflow-y: auto;
  padding: 20px 0;
  position: sticky;
  top: 0;
  height: 100vh;
}

#toc-label {
  font-size: 10px;
  font-weight: 600;
  letter-spacing: .12em;
  text-transform: uppercase;
  color: var(--text-dim);
  padding: 0 18px 10px;
}

#toc a {
  display: block;
  padding: 4px 18px;
  font-size: 12.5px;
  color: var(--text-muted);
  text-decoration: none;
  transition: color .15s, background .15s;
  line-height: 1.5;
  border-left: 2px solid transparent;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}

#toc a:hover {
  color: var(--blue);
  background: var(--blue-pale);
  border-left-color: var(--blue-light);
}

#toc a.h1 {
  font-weight: 600;
  color: var(--text);
  padding-left: 18px;
  margin-top: 6px;
}

#toc a.h2 {
  padding-left: 28px;
}

#toc a.h3 {
  padding-left: 40px;
  font-size: 11.5px;
}

/* ---- Main Content ---- */
#notebook {
  min-width: 0;
  width: 100%;
}

/* ================================================================
   Prose Blocks (shared)
   ================================================================ */
.block-module-doc,
.block-doc-comment {
  font-family: var(--font-prose);
  font-size: 17px;
  line-height: 1.85;
  color: var(--text);
  margin-bottom: 10px;
}

/* --- module-doc: white card --- */
.block-module-doc {
  padding: 32px 36px;
  background: var(--surface);
  border: 1px solid var(--border-soft);
  border-radius: var(--radius);
  box-shadow: var(--shadow-sm);
}

/* --- doc-comment: left blue border --- */
.block-doc-comment {
  padding: 16px 20px;
  border-left: 3px solid var(--blue-light);
  background: var(--blue-pale);
  border-radius: 0 var(--radius) var(--radius) 0;
}

/* ================================================================
   Prose Typography (shared)
   ================================================================ */
.block-module-doc h1,
.block-doc-comment h1,
.block-doc-comment h1,
.block-doc-comment h1 {
  font-size: 1.9em;
  font-weight: 600;
  color: var(--blue-dark);
  border-bottom: 2px solid var(--blue-pale);
  padding-bottom: 10px;
  margin: 0 0 20px;
  line-height: 1.3;
}

.block-module-doc h2,
.block-doc-comment h2,
.block-doc-comment h2,
.block-doc-comment h2 {
  font-size: 1.35em;
  font-weight: 600;
  color: var(--blue);
  margin: 24px 0 14px;
  line-height: 1.4;
}

.block-module-doc h3,
.block-doc-comment h3,
.block-doc-comment h3,
.block-doc-comment h3 {
  font-size: 1.1em;
  font-weight: 600;
  color: var(--text);
  margin: 18px 0 10px;
}

.block-module-doc h4,
.block-doc-comment h4,
.block-doc-comment h4,
.block-doc-comment h4 {
  font-size: 1em;
  font-weight: 600;
  color: var(--text-muted);
  margin: 14px 0 8px;
}

.block-module-doc p,
.block-doc-comment p,
.block-doc-comment p,
.block-doc-comment p {
  margin: 0 0 12px;
}

/* Remove bottom margin from the last child to keep padding visually equal */
.block-module-doc>*:last-child,
.block-doc-comment>*:last-child,
.block-doc-comment>*:last-child,
.block-doc-comment>*:last-child {
  margin-bottom: 0;
}

/* Remove top margin from the first child to keep padding visually equal */
.block-module-doc>*:first-child,
.block-doc-comment>*:first-child,
.block-doc-comment>*:first-child,
.block-doc-comment>*:first-child {
  margin-top: 0;
}

.block-module-doc strong,
.block-doc-comment strong,
.block-doc-comment strong,
.block-doc-comment strong {
  color: var(--text);
  font-weight: 600;
}

.block-module-doc em,
.block-doc-comment em,
.block-doc-comment em,
.block-doc-comment em {
  color: var(--text-muted);
  font-style: italic;
}

.block-module-doc ul,
.block-module-doc ol,
.block-doc-comment ul,
.block-doc-comment ol,
.block-doc-comment ul,
.block-doc-comment ol,
.block-doc-comment ul,
.block-doc-comment ol {
  margin: 8px 0 14px 22px;
}

.block-module-doc li,
.block-doc-comment li,
.block-doc-comment li,
.block-doc-comment li {
  margin-bottom: 5px;
}

.block-module-doc li p,
.block-doc-comment li p,
.block-doc-comment li p,
.block-doc-comment li p {
  margin: 0;
}

/* Inline code */
.block-module-doc code,
.block-doc-comment code,
.block-doc-comment code,
.block-doc-comment code {
  font-family: var(--font-code);
  font-size: .84em;
  background: var(--surface-alt);
  border: 1px solid var(--border);
  border-radius: 3px;
  padding: 1px 5px;
  color: var(--blue-dark);
}

/* Code fences */
.block-module-doc pre,
.block-doc-comment pre,
.block-doc-comment pre,
.block-doc-comment pre {
  background: var(--code-bg);
  border: 1px solid var(--border);
  border-radius: var(--radius);
  padding: 14px 16px;
  overflow-x: auto;
  margin: 12px 0;
  font-family: var(--font-code);
  font-size: 13px;
  line-height: 1.7;
  tab-size: 2;
}

.block-module-doc pre code,
.block-doc-comment pre code,
.block-doc-comment pre code,
.block-doc-comment pre code {
  background: none;
  border: none;
  padding: 0;
  color: var(--text);
  font-size: inherit;
}

/* Tables */
.block-module-doc table,
.block-doc-comment table,
.block-doc-comment table,
.block-doc-comment table {
  border-collapse: collapse;
  width: 100%;
  margin: 16px 0;
  font-family: var(--font-ui);
  font-size: 14px;
}

.block-module-doc th,
.block-doc-comment th,
.block-doc-comment th,
.block-doc-comment th {
  background: var(--surface-alt);
  color: var(--blue-dark);
  font-weight: 600;
  padding: 8px 14px;
  text-align: left;
  border: 1px solid var(--border);
}

.block-module-doc td,
.block-doc-comment td,
.block-doc-comment td,
.block-doc-comment td {
  padding: 7px 14px;
  border: 1px solid var(--border);
  color: var(--text);
}

.block-module-doc tr:nth-child(even) td,
.block-doc-comment tr:nth-child(even) td,
.block-doc-comment tr:nth-child(even) td,
.block-doc-comment tr:nth-child(even) td {
  background: #f0f5fd;
}

/* Blockquotes */
.block-module-doc blockquote,
.block-doc-comment blockquote,
.block-doc-comment blockquote,
.block-doc-comment blockquote {
  border-left: 3px solid var(--blue-dim);
  padding-left: 16px;
  margin: 12px 0;
  color: var(--text-muted);
  font-style: italic;
}

.block-module-doc hr,
.block-doc-comment hr,
.block-doc-comment hr,
.block-doc-comment hr {
  border: none;
  border-top: 1px solid var(--border);
  margin: 24px 0;
}

.block-module-doc a,
.block-doc-comment a,
.block-doc-comment a,
.block-doc-comment a {
  color: var(--blue);
  text-decoration: none;
}

.block-module-doc a:hover,
.block-doc-comment a:hover,
.block-doc-comment a:hover,
.block-doc-comment a:hover {
  text-decoration: underline;
}

/* ================================================================
   Code Blocks
   ================================================================ */
.block-code {
  margin-bottom: 10px;
  border-radius: var(--radius);
  overflow: hidden;
  border: 1px solid var(--border);
  box-shadow: var(--shadow-sm);
}

.block-code-header {
  background: var(--surface-alt);
  padding: 7px 16px;
  font-family: var(--font-code);
  font-size: 11px;
  color: var(--text-dim);
  border-bottom: 1px solid var(--border);
  display: flex;
  align-items: center;
  gap: 8px;
}

.block-code-header::before {
  content: '';
  display: inline-block;
  width: 7px;
  height: 7px;
  border-radius: 50%;
  background: var(--blue-light);
  opacity: .7;
}

.lean-source {
  background: var(--code-bg);
  margin: 0;
  padding: 18px 20px;
  font-family: var(--font-code);
  font-size: 13.5px;
  line-height: 1.7;
  white-space: pre;
  overflow-x: auto;
  tab-size: 2;
  color: var(--text);
}

/* ================================================================
   Syntax Highlighting
   ================================================================ */
.hl-keyword {
  color: var(--hl-keyword);
  font-weight: 600;
}

.hl-tactic {
  color: var(--hl-tactic);
}

.hl-type {
  color: var(--hl-type);
}

.hl-string {
  color: var(--hl-string);
}

.hl-number {
  color: var(--hl-number);
}

.hl-comment {
  color: var(--hl-comment);
  font-style: italic;
}

.hl-op {
  color: var(--hl-op);
}

/* ================================================================
   Mermaid Blocks
   ================================================================ */
.block-mermaid {
  background: var(--surface);
  border: 1px solid var(--border);
  border-radius: var(--radius);
  padding: 28px 24px;
  margin-bottom: 10px;
  overflow-x: auto;
  overflow-y: visible;
  box-shadow: var(--shadow-sm);
}

.block-mermaid svg {
  height: auto;
}

.block-mermaid .error {
  color: #c00;
  padding: 1em;
  border-radius: 4px;
  text-align: left;
}

/* ================================================================
   Graphviz (DOT) Blocks
   ================================================================ */
.block-graphviz {
  background: var(--surface);
  border: 1px solid var(--border);
  border-radius: var(--radius);
  padding: 28px 24px;
  margin-bottom: 10px;
  overflow-x: auto;
  overflow-y: visible;
  box-shadow: var(--shadow-sm);
}

.block-graphviz svg {
  height: auto;
}

.block-graphviz .error {
  color: #c00;
  padding: 1em;
  border-radius: 4px;
  text-align: left;
}

/* ================================================================
   MathJax
   ================================================================ */
mjx-container {
  color: inherit !important;
}

mjx-container[display="true"] {
  display: block !important;
  overflow: visible !important;
  margin: 0 !important;
  max-width: 100%;
  font-size: 1.15em;
}

.mjx-display-wrap {
  overflow-x: auto;
  overflow-y: hidden;
  margin: 20px 0;
  padding: 4px 0;
  -webkit-overflow-scrolling: touch;
  scrollbar-width: thin;
  scrollbar-color: var(--blue-dim) transparent;
}

.mjx-display-wrap::-webkit-scrollbar {
  height: 4px;
}

.mjx-display-wrap::-webkit-scrollbar-track {
  background: transparent;
}

.mjx-display-wrap::-webkit-scrollbar-thumb {
  background: var(--blue-dim);
  border-radius: 2px;
}

/* Heading scroll offset */
h1,
h2,
h3,
h4,
h5,
h6 {
  scroll-margin-top: 72px;
}

/* ================================================================
   Scrollbar
   ================================================================ */
::-webkit-scrollbar {
  width: 6px;
  height: 6px;
}

::-webkit-scrollbar-track {
  background: transparent;
}

::-webkit-scrollbar-thumb {
  background: var(--blue-dim);
  border-radius: 3px;
}

/* ================================================================
   Lean LSP Output (#eval / proof status)
   ================================================================ */
.lean-output {
  border-top: 1px solid var(--border);
  padding: 10px 10px 10px 22px;
  background: rgba(0, 0, 0, .02);
  position: relative;
}

.lean-output::before {
  content: "";
  position: absolute;
  left: 0;
  top: 0;
  bottom: 0;
  width: 4px;
  background: var(--blue);
}

.output-label {
  display: block;
  font-size: .78em;
  color: var(--text-muted);
  font-weight: 600;
  margin-bottom: 4px;
  text-transform: uppercase;
}

.lean-output pre {
  margin: 0;
  font-family: var(--font-code);
  white-space: pre-wrap;
  font-size: 13px;
}
  </style>
</head>

<body>
  <script type="text/x-lean-source" id="lean-source">import CL8E8TQC._20_FTQC_GP_ML._04_ActiveLearning

namespace CL8E8TQC.QuantumDeepGP.LazyTraining

/-!
# Quantum Deep Gaussian Processes via Topological Quantum Computation

## Abstract

Infinite-width neural networks (NN) become equivalent to Gaussian processes (GP) (Neal 1996, Jacot et al. 2018 NTK theory), but at this limit the kernel freezes to its initial value, causing "Lazy Training" where representation learning capability is lost. This module first rigorously diagnoses, based on NTK theory, that CL8E8TQC's $O(n)$ exact GP belongs to the Lazy Regime (static kernel model), acknowledges this constraint, and then presents two complementary strategies. First, exact uncertainty enables Active Learning that transforms Lazy from a lack of expressiveness into a search efficiency advantage. Second, breaking through the $O(n^3)$ computational barrier to $O(n)$ makes Deep GP (representation learning via multi-layering) feasible at realistic computational cost for the first time, opening the path from Lazy Training to Rich Regime. Subsequent modules `_01`–`_04` fully develop this Quantum Deep GP.

## 1. Introduction

The equivalence theorem "infinite-width NN = GP" was proved by Neal (1996) for 1 hidden layer and extended to arbitrary depth by Jacot et al. (2018). This theorem means that even when training an infinite-width NN, the Neural Tangent Kernel $\Theta_t(x,x') = \nabla_\theta f(x;\theta_t)^T \nabla_\theta f(x';\theta_t)$ does not change from initialization ($\Theta_t \approx \Theta_0$). Individual parameters move only $O(1/\sqrt{m})$, and since no feature space distortion occurs, the network cannot rewrite its internal representation upon seeing data — entering a "lazy training" state.

Meanwhile, in practical finite-width NNs (ResNet, Transformer, etc.), parameters move significantly because width is finite, and $\Theta_t$ changes dramatically in response to data — entering the "Rich Regime (representation learning regime)." This is the fundamental reason infinite-width GP is inferior to finite-width NN in practical performance. CL8E8TQC single-layer GP has computational superiority with $O(n)$ exact inference, but since kernel $k(x,y)$ is a fixed static model, it is classified in the Lazy Regime.

The Quantum Deep GP presented in this module constructs an effective kernel via multi-layering: $k_{\text{eff}}^{(L)}(x,y) = \sum_{\vec{h} \in H84^L} \prod_{l=0}^{L} k(h_l, h_{l+1})$, achieving $\partial k_{\text{eff}}^{(L)} / \partial x \neq 0$. This corresponds to breaking the discrete counterpart of NTK theory's Lazy condition $\partial\Theta/\partial t = 0$, representing a structural transition to Rich Regime. The variation is in the depth direction (GP multi-layering) rather than the time direction (NN training), but the essence of acquiring input-dependent nonlinear transformations is shared.

## 2. Relationship to Prior Work

| Prior Work | Content | Relationship to This Module |
|:---|:---|:---|
| Neal (1996) *Bayesian Learning for NN* | Convergence proof: infinite-width 1-hidden-layer NN = GP | Theoretical explanation of Lazy Training's origin |
| Jacot et al. (2018) NTK | NTK extension to arbitrary depth / establishment of Lazy Training | Primary source for §5's NTK theory |
| Lee et al. (2019) | Rich/Lazy Regime discrimination criteria for finite-width NN | Criteria for CL8E8TQC GP's Lazy diagnosis |
| Damianou & Lawrence (2013) | Deep GP original / $O(Ln^3)$ complexity / variational inference | `_01` breaks through to $O(Ln^3) \to O(Ln)$ |
| `_20_FTQC_GP_ML/_04_ActiveLearning` | $O(n)$ single-layer GP / function identification with 9 points | Inherited as exploration strategy for Lazy GP |

## 3. Contributions of This Chapter

- **Rigorous Lazy Training diagnosis**: Explicitly identify CL8E8TQC single-layer GP as static kernel model (Lazy Regime) based on NTK theory
- **Structural path to Rich Regime**: Argument for input-dependence of effective kernel via multi-layering $\partial k_{\text{eff}}^{(L)}/\partial x \neq 0$
- **Formulation of two complementary strategies**: ①Active Learning via exact uncertainty (search optimization) ②$O(n)$ Quantum Deep GP (representation learning)
- **Formal connection between NTK theory and Quantum Deep GP**: Argument for correspondence: discrete routing ≡ Rich Regime
- **Theoretical foundation for subsequent modules `_01`–`_04`**: Lazy diagnosis is the starting point for Deep GP theory, path integrals, and BQP completeness

## 4. Chapter Structure

| Section | Title | Content |
|:---|:---|:---|
| §1 | Problem Setting | Paradox: "If infinite-width NN = GP, why is GP inferior to NN?" |
| §2 | Concept of Lazy Training | Kernel freezing / feature space stasis mechanism |
| §3 | Adaptive vs Static Kernels | Essential contrast: finite-width NN (Rich) vs infinite-width GP (Lazy) |
| §4 | Difference Between Standard GP and NNGP | Computational cost comparison: practical GP vs NTK/NNGP |
| §5 | NTK Theory | Formulation of $\Theta_t(x,x')$ / mathematical mechanism of Lazy Training |
| §6 | Lazy Diagnosis of CL8E8TQC GP | Explicit Lazy certification / two complementary strategies / formal connection to Rich Regime |

Central results are as follows:

1. **Lazy Training diagnosis** (this file):
   Analyze the kernel freezing mechanism in infinite-width NN=GP based on NTK theory, and rigorously determine that CL8E8TQC GP is a static kernel model (Lazy Regime).

2. **Deep GP theory and $O(Ln^3) \to O(Ln)$ breakthrough** (`_01`):
   Reconstruct Damianou-Lawrence (2013) Deep GP with CL8E8TQC's rank-bounded kernel, achieving $O(n)$ exact inference per layer.

3. **Elimination of variational inference via discrete path integral** (`_02`):
   Discretization to $H84$ codewords (not approximation but exact exhaustive enumeration) reduces inter-layer marginalization integral to 16-term finite sums, completely eliminating variational inference.

4. **Quantum interference and BQP structure** (`_03`):
   When GoldenGate kernel ($C^6$, Non-Clifford) is used for hidden layer transitions, positive-negative mixed interference (discrete manifestation of quantum interference) emerges between paths, and the inference computation is proved as theorem to belong to BQP class.

5. **Layer = quantum depth equation** (`_04`):
   Show that Quantum Deep GP layers and TQC circuit depth reduce to the same Fusion+Braiding operations, deriving the equation $\text{Quantum Deep GP} \equiv \text{TQC}$.

**Keywords**: quantum-deep-gp, lazy-training, ntk, deep-gp, variational-inference, forbidden-float, h84-codeword, golden-gate, bqp, topological-quantum-computation

## Prerequisites — Established Theoretical Foundations Inherited by This Module

This module `_21_QuantumDeepGP` is constructed **fully assuming and inheriting** the theoretical results of the following two preceding modules. Readers are expected to have read these before proceeding.

### Foundation 1: `_01_TQC` — Cl(8) Algebra and Topological Quantum Computation

| File | Established Result |
|:---|:---|
| `_01_Cl8E8H84` | Cl(8) algebra's 256 bases, $H(8,4)$ code's 16 codewords, `geometricProduct`'s XOR+sign operation |
| `_02_PinSpin` | Pin/Spin groups, spinor reflection `reflect` constructive implementation |
| `_03_QuantumState` | 256-dimensional integer-valued discrete wavefunction `QuantumState`, interference Level 1 (positive-negative coexistence of amplitude components) |
| `_04_TQC_Universality` | GoldenGate $C^6$ (Non-Clifford, order 5) quantum computational universality, BPP→BQP |
| `_05_FTQC` | Norm preservation over 100-stage gate application (zero-error FTQC) |

### Foundation 2: `_20_FTQC_GP_ML` — $O(n)$ Exact Gaussian Process

| File | Established Result |
|:---|:---|
| `_00_LinearTimeGP` | $O(n)$ exact GP inference via Hamming kernel (rank $\le$ 8), Bareiss algorithm, `featureMap`, `e8Kernel` |
| `_01_KernelCatalog` | Systematic catalog of E8 kernel family, grade-wise kernel properties |
| `_02_DrugDiscovery_GP` | Concrete application to drug discovery, comparison with Tanimoto similarity |
| `_03_MultiE8_GP` | Multi-E8 kernel composition method |
| `_04_ActiveLearning` | Active learning based on exact uncertainty, function identification with 9 points |

### Subsequent to This Module: `_22_ExactDeepBayesianOptimization`

The Quantum Deep GP established in this module is applied and extended in:

| File | Application of This Module's Results |
|:---|:---|
| `_22/_00_ExactDeepBO` | Integrates Deep Feature Map with Woodbury reduction to achieve exact Deep BO |
| `_22/_01_NN_vs_GP` | Uses Lazy Training diagnosis (this file §6) as foundation for complete GP vs NN comparison |

### Logical Structure of Inheritance

$$\underbrace{\text{Cl(8), H84, GoldenGate}}_{\text{\_00\_TQC}}
\;\xrightarrow{\text{kernel construction}}\;
\underbrace{O(n)\text{ exact GP}}_{\text{\_20\_FTQC\_GP\_ML}}
\;\xrightarrow{\text{multi-layering}}\;
\underbrace{\text{Quantum Deep GP}}_{\text{\_21\_QuantumDeepGP (this module)}}$$

What this module newly introduces is the theory of **multi-layering** (Deep-ification). Specifically:

- Stack the **$O(n)$ single-layer GP** established in `_20` into multiple layers
- Use `_20`'s `e8Kernel` (Hamming) and `_01_TQC`'s `goldenKernel` (GoldenGate) as **inter-layer transition kernels**
- Analyze **inter-path interference** (a new phenomenon absent in single-layer GP) that emerges with multi-layering, and discuss connection to computational complexity class BQP

---

# §1. Problem Setting — If Infinite-Width NN Equals GP, Why Is GP Inferior to NN?

Infinite-dimensional (infinite-width) neural networks (NN) become Gaussian processes (GP). GP is the computationally expensive superstructure of NN, and there appears to be no reason for inferiority.

**"A neural network with infinitely many hidden layer units (infinite-width NN) becomes mathematically equivalent to a Gaussian process (GP)"** — this is a fact fully proven since Radford Neal's 1996 proof, through recent **Neural Tangent Kernel (NTK)** and **NNGP (Neural Network Gaussian Process)** theories.

Neal (1996) proved this GP convergence for **1 hidden layer** NN, and Jacot et al. (2018)'s NTK theory provided extension to **arbitrary depth**. Below, we distinguish these two results.

"Since infinite-width NN = GP, GP is a strictly superior superset that includes NN; there should be no inferiority" — this logic is theoretically entirely correct.

Then why does the paradox arise: "In real machine learning tasks (images, language, etc.), GP (infinite-width NN) is inferior to the finite-width NN we normally use?"

**The moment you take width to infinity to become GP, NN's greatest weapon — "Feature Learning" capability — dies.**

---

# §2. What Happens When Going Infinite-Width (GP): "Lazy Training"

The NN we normally use (finite-width), as training progresses, significantly changes layer weights and acquires optimal representations by distorting "the feature space itself" to fit data (representation learning).

However, when theoretically taking NN width to infinity, a surprising phenomenon occurs.

Each neuron's weight change becomes infinitesimal, and **"from the point of initialization, the feature space (kernel) does not move at all even as training progresses."**

In deep learning theory, this state is called **"Lazy Training (Lazy Learning Regime)."**

Infinite-width NN (= GP) appears to be learning parameters, but in reality degenerates into mere linear regression (kernel method) using "a fixed kernel (NTK or NNGP kernel) determined at initialization."

---

# §3. "Adaptive Kernel" vs "Static Kernel"

Here lies the true identity of GP's "structural inferiority" to NN.

**Finite-width NN (real NN):**

While observing data, **dynamically (adaptively)** rewrites internal feature representations (kernel). For images: hierarchically learns features from edges to cat ear shapes.

**Infinite-width NN (= GP / NNGP / NTK):**

Mathematically a superior structure with enormous computation, but the kernel is **static (fixed)**. No matter how much data is given, it must fight with only "the geometric prior knowledge inherent in the initial network structure" and cannot optimize feature representations to fit data.

---

# §4. Difference Between Standard GP and NNGP

There is another reason for "GP's inferiority" in practical terms.

The GP kernels theoretically equivalent to NN (NNGP kernel, NTK) have extremely complex formulas and astronomical computational cost, so in practice one must approximate the computation or run only on small datasets.

Therefore, models generally used as "GP" employ **simple general-purpose kernels** like RBF or Matérn kernels that depend only on Euclidean distance between data. Without powerful structures (inductive biases) like "locality" and "ordering" found in CNNs and Transformers, they cannot compete with complex data like raw images.

## Summary

GP is undoubtedly the superstructure that is the "infinite-width limit" of NN. However, **as the price of taking the infinite limit, it loses "the ability to dynamically learn representations" and becomes a fixed kernel**, so for complex real data, it loses in performance to NN that deliberately stays "finite" to perform feature learning.

The moment it obtains "the strongest spear" of infinity, it no longer needs to move and becomes "just a fixed mirror (GP)." This is the fundamental reason infinite-width NN (GP) is inferior in performance to finite-width NN on real tasks.

---

# §5. NTK (Neural Tangent Kernel) Theory — Mathematical Mechanism Elucidation

We enter the world of "NTK theory" — one of the most beautiful yet counter-intuitive theories in deep learning.

We unravel through mathematical dynamics the paradox: "Why does infinitely increasing parameters (width) cause learning to become lazy?"

## 5.1 Formulating NN Learning as a "Kernel"

Let $f(x; \theta)$ be the NN output for input $x$ ($\theta$ denotes all parameters: weights, biases, etc.).

As parameters $\theta$ are updated over time $t$ through learning (gradient descent), the output $f(x; \theta)$ changes accordingly. The **NTK (Neural Tangent Kernel)** is the following kernel function describing "how easily the output changes":

$$\Theta_t(x, x') = \nabla_\theta f(x; \theta_t)^T \nabla_\theta f(x'; \theta_t)$$

This formula represents "when parameters are moved to correct error at data $x'$, how much the prediction at different data $x$ is pulled along."

In other words, **$\Theta_t(x, x')$ is the NN's "current way of perceiving feature space (measuring similarity between data)" itself.**

## 5.2 Infinite-Width Limit: Emergence of Lazy Training

Here is the crux. Take the hidden layer width (number of neurons) $m$ to infinity ($m \to \infty$).

With appropriate initialization (scaling weights by $1/\sqrt{m}$, etc.) to prevent NN output from diverging, a surprising phenomenon occurs.

Because width is infinite, to bring the loss function to zero (complete training), **individual parameters $\theta$ need to move only slightly ($O(1/\sqrt{m})$).** An infinite accumulation of tiny effects.

Individual parameters barely moving from initial values means the function gradient $\nabla_\theta f$ also doesn't change from the initial state. Consequently, the kernel during training $\Theta_t$ becomes frozen at the initialization-time kernel $\Theta_0$.

$$\Theta_t(x, x') \approx \Theta_0(x, x') \quad (\text{for all } t)$$

This is the true identity of **"Lazy Training."** Infinite-width NN (= GP) appears to learn parameters, but is actually "just performing linear regression (kernel method) using the fixed kernel $\Theta_0$ that happened to form at initialization." It completely abandons distorting feature space to fit data (learning representations).

## 5.3 Real NN (Finite-Width): The Power of Feature Learning

Meanwhile, what happens in actually used finite-width NNs (ResNet, Transformer, etc.)?

Since width $m$ is finite, **individual parameters must move significantly (macroscopically)** to reduce loss. When parameters $\theta$ move significantly, naturally the kernel $\Theta_t$ changes dramatically over time.

1. **Early training:** Kernel $\Theta_0$ is random, measuring meaningless similarity.

2. **Training progression:** Parameters move significantly to capture data features (image contours, word contexts, etc.).

3. **Late training:** Kernel $\Theta_t$ becomes optimized for data, acquiring a **powerful unique feature space** that can determine "data of the same class are similar."

In the theory community, this state is called **"Rich Regime"** or **"Feature Learning Regime"** in contrast to Lazy Training. Because width is finite, there is "need to work hard moving parameters," and as a result, intelligent feature representations are acquired.

The approach of actually measuring and confirming on a given model whether it is "lazy" or "in Rich representation learning" (measuring representation change using CKA, etc.) is also being researched.

---

# §6. Rigorous Determination of Lazy Training in CL8E8TQC GP

In this theory's discrete GP as well, the drawback of infinite-width NN — "Lazy Training (the lack of representation learning capability)" — is **structurally unavoidable and fully present**.

However, this theory's characteristic is that it does not hide this, but rather **acknowledges it as a "clear constraint (freezing)" and then attempts to complement and break through that weakness with entirely different approaches**.

## 6.1 Rigorous Determination: The Kernel Is "Frozen"

CL8E8TQC GP achieves ultra-fast, exact inference at $O(n)$, but as the cost (premise), it **retains GP's inherent property that "the kernel (how data similarity is measured) is fixed."**

In `_22/_01_NN_vs_GP.lean` §3.1 "Representation Learning: Lazy Training → Rich Regime" (citing this module's diagnostic results), the following is explicitly stated:

- "GP's greatest constraint is that kernel $k(x,y)$ is fixed a priori"

- "The E8 kernel ... is mathematically beautiful, but inductive bias is **frozen**"

- "Does CL8E8TQC solve representation learning? — **Directly, No.**"

That is, adaptive representation learning where internal feature representations (kernel) are dynamically rewritten while observing data, like NN (finite-width), **does not occur.** This state is essentially identical to the "Lazy Training" of infinite-width NN (NTK) where the initialized kernel stops moving.

## 6.2 This Theory's Response — Two Alternative Approaches

This theory uses two weapons — **"breaking the computational barrier"** and **"exact uncertainty"** — against the weakness that "the kernel does not adaptively move (is Lazy)," presenting the following alternative routes.

### ① Competing via "Search Optimization" Rather Than "Representation Learning"

While NN "rewrites the map (feature space) to fit data," this theory's GP takes the stance of "the map is fixed (Lazy), but possesses an **absolutely reliable compass (complete confidence computation)**."

As demonstrated in `_04_ActiveLearning.lean`, since near-zero-approximation exact `uncertainty` can be computed, when running Active Learning loops, it can pinpoint unexplored regions with efficiency impossible for NN (theoretically, just 9 data points for L=1 to completely identify the function).

### ② Breaking the $O(n^3)$ Wall and Opening the Path to "Quantum Deep GP"

This is the most ambitious point of this theory.

Previously, even attempting to stack GPs in layers for hierarchical representation learning like NN (Deep GP), computational complexity exploded to $O(Ln^3)$, making it practically impossible.

However, this theory succeeded in reducing GP computation to **$O(n)$** using Cl(8) algebra properties. This means **"composing multiple kernels" and "multi-layering GP (Quantum Deep GP)" — research to acquire representation learning within the GP framework — has become feasible at realistic computational cost for the first time**.

### ③ Formal Connection to NTK Theory — Discrete Routing ≡ Rich Regime

In NTK theory, the diagnostic condition for Lazy Training was $\partial \Theta_t / \partial t = 0$ (kernel is time-invariant) (see §5).

Quantum Deep GP's discrete routing can be shown to **structurally break** this condition as follows:

The single-layer CL8E8TQC GP effective kernel is $k_{\text{eff}}(x,y) = k(x,y)$, fixed (Lazy). However, the multi-layer Quantum Deep GP effective kernel is:

$$k_{\text{eff}}^{(L)}(x,y) = \sum_{\vec{h} \in H84^L}
\prod_{l=0}^{L} k(h_l, h_{l+1})$$

This composite kernel **changes nonlinearly depending on input $x$**. Specifically, when $x$ is close to H84 codeword $c_1$ versus $c_{10}$, the routing path weight distribution changes, so the effective kernel implicitly varies data-dependently.

$$\frac{\partial k_{\text{eff}}^{(L)}}{\partial x} \neq 0$$

This means the discrete counterpart of NTK theory's Lazy condition $\partial \Theta / \partial t = 0$ does not hold, providing formal basis that Quantum Deep GP's discrete routing **belongs to Rich Regime (representation learning state)**.

However, while NN's Rich Regime has "kernel changing during training," Quantum Deep GP has "kernel changing by stacking layers" — a structural difference. The former is variation in time direction, the latter in depth direction, but both share the essence of **escaping kernel fixation and acquiring input-dependent nonlinear transformations**.

## 6.3 Conclusion

This theory's discrete GP itself, viewed alone, remains a static kernel model in "Lazy Training" state.

However, this theory's true contribution is not directly fixing Lazy, but **"pulling 'quantum Deep-ification of GP (Quantum Deep GP),' which was previously untouchable due to $O(n^3)$ complexity, down onto the $O(n)$ playing field."**

This opens the path for GP-based models to acquire "representation learning (hierarchy)" rivaling NN while maintaining exact confidence.

The next `_01_DeepGP_Theory` discusses the theoretical foundation and computational barriers of this Quantum Deep GP in detail.

---

## References

### Lazy Training / NTK Theory
- Neal, R.M. (1996). *Bayesian Learning for Neural Networks*,
  Lecture Notes in Statistics, Springer.
  (Original source of infinite-width NN = GP convergence theorem)
- Jacot, A., Gabriel, F. and Hongler, C. (2018).
  "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
  *NeurIPS 2018*.
  (NTK extension to arbitrary depth / theoretical establishment of Lazy Training)
- Lee, J. et al. (2019).
  "Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent",
  *NeurIPS 2019*.
  (Rich Regime / Lazy Regime discrimination criteria for finite-width NN)

### Deep GP / Uncertainty Estimation
- Damianou, A. and Lawrence, N.D. (2013).
  "Deep Gaussian Processes", *AISTATS 2013*.
  (Deep GP original: $O(Ln^3)$ complexity and introduction of variational inference)
- Lakshminarayanan, B., Pritzel, A. and Blundell, C. (2017).
  "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
  *NeurIPS 2017*.
- Gal, Y. and Ghahramani, Z. (2016).
  "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
  *ICML 2016*.

### Module Connections
- **Previous**: `_20_FTQC_GP_ML/_04_ActiveLearning.lean` — $O(n)$ single-layer GP / Active Learning
- **Next**: `_01_DeepGP_Theory.lean` — Deep GP computational barrier $O(Ln^3)$ and breakthrough
- **Next**: `_22_ExactDeepBayesianOptimization/_01_NN_vs_GP.lean` — Lazy Training diagnosis referenced as foundation for GP vs NN comparison

-/

end CL8E8TQC.QuantumDeepGP.LazyTraining
</script>

  <div id="app">
    <div id="topbar">
      <div class="logo">lean<span> notebook</span></div>
      <div class="sep">·</div>
      <div class="doc-title" id="doc-title">Loading…</div>
      <div id="view-toggle">
        <input type="radio" name="view" id="vlean" value="lean">
        <label for="vlean">lean</label>
        <input type="radio" name="view" id="vhtml" value="html" checked>
        <label for="vhtml">HTML</label>
      </div>
    </div>
    <nav id="sidebar">
      <div id="toc-label">Contents</div>
      <div id="toc"></div>
    </nav>
    <main id="notebook"></main>
    <div id="lean-raw">
      <pre id="lean-raw-pre"></pre>
    </div>
  </div>

  <script>
// ================================================================
// renderer.js — Shared rendering logic for LeanNotebook
// Used by both the VSCode WebView (main.js) and HTML export (template.html).
// DO NOT add VSCode-specific or VanJS-specific code here.
// ================================================================

// ----------------------------------------------------------------
// Lean 4 Syntax Highlighter
// ----------------------------------------------------------------
const LR_KW = new Set([
    'def', 'abbrev', 'theorem', 'lemma', 'example', 'noncomputable',
    'private', 'protected', 'instance', 'class', 'structure', 'inductive', 'where', 'with',
    'extends', 'deriving', 'namespace', 'end', 'section', 'open', 'import', 'export',
    'universe', 'variable', 'attribute', 'notation', 'macro', 'syntax', 'elab',
    'by', 'do', 'return', 'let', 'have', 'show', 'from', 'fun', 'match', 'if', 'then', 'else',
    'for', 'while', 'mut', 'pure', 'calc', 'suffices', 'obtain', 'refine', 'exact', 'apply',
    'intro', 'intros', 'cases', 'induction', 'constructor', 'use', 'rfl', 'simp', 'ring',
    'omega', 'linarith', 'norm_num', 'decide', 'native_decide', 'trivial', 'assumption',
    'contradiction', 'aesop', 'tauto', 'field_simp', 'push_neg', 'pull_neg',
    'partial', 'unsafe', 'opaque', 'axiom'
]);
const LR_TY = new Set([
    'Nat', 'Int', 'Bool', 'String', 'Float', 'Char', 'UInt8', 'UInt16',
    'UInt32', 'UInt64', 'Int8', 'Int16', 'Int32', 'Int64', 'List', 'Array', 'Vector',
    'Option', 'Result', 'IO', 'Type', 'Prop', 'Sort', 'Unit', 'Empty', 'True', 'False',
    'Eq', 'And', 'Or', 'Not', 'Iff', 'Exists', 'Sigma', 'Subtype', 'Fin', 'BitVec'
]);
const LR_TA = new Set([
    'native_decide', 'decide', 'rfl', 'simp', 'ring', 'omega',
    'linarith', 'norm_num', 'exact', 'apply', 'intro', 'intros', 'cases', 'rcases',
    'induction', 'constructor', 'use', 'refine', 'suffices', 'obtain', 'contradiction',
    'trivial', 'assumption', 'aesop', 'tauto', 'field_simp', 'push_neg', 'pull_neg',
    'positivity', 'norm_cast', 'push_cast', 'ext', 'funext', 'congr', 'conv', 'rw',
    'rewrite', 'gcongr', 'abel'
]);

function lrEsc(s) {
    return s.replace(/&/g, '&amp;').replace(/</g, '&lt;').replace(/>/g, '&gt;');
}

function lrHlLine(raw) {
    let cmt = -1, inStr = false;
    for (let i = 0; i < raw.length - 1; i++) {
        if (raw[i] === '"' && (i === 0 || raw[i - 1] !== '\\')) inStr = !inStr;
        if (!inStr && raw[i] === '-' && raw[i + 1] === '-') { cmt = i; break; }
    }
    const codePart = cmt >= 0 ? raw.slice(0, cmt) : raw;
    const tailPart = cmt >= 0 ? raw.slice(cmt) : '';
    let out = '', i = 0;
    while (i < codePart.length) {
        const ch = codePart[i];
        if (ch === '"') {
            let j = i + 1;
            while (j < codePart.length && (codePart[j] !== '"' || codePart[j - 1] === '\\')) j++;
            out += `<span class="hl-string">${lrEsc(codePart.slice(i, j + 1))}</span>`;
            i = j + 1; continue;
        }
        if (ch === '0' && i + 1 < codePart.length && (codePart[i + 1] === 'b' || codePart[i + 1] === 'x')) {
            let j = i + 2;
            while (j < codePart.length && /[0-9a-fA-F_]/.test(codePart[j])) j++;
            let sf = '';
            if (j < codePart.length && codePart[j] === '#') {
                let k = j + 1;
                while (k < codePart.length && /\d/.test(codePart[k])) k++;
                sf = lrEsc(codePart.slice(j, k)); j = k;
            }
            out += `<span class="hl-number">${lrEsc(codePart.slice(i, j))}${sf}</span>`;
            i = j; continue;
        }
        if (/\d/.test(ch) && (i === 0 || !/\w/.test(codePart[i - 1]))) {
            let j = i;
            while (j < codePart.length && /[\d_]/.test(codePart[j])) j++;
            out += `<span class="hl-number">${lrEsc(codePart.slice(i, j))}</span>`;
            i = j; continue;
        }
        if (/[a-zA-Z_]/.test(ch) || ch.charCodeAt(0) > 127) {
            let j = i + 1;
            while (j < codePart.length && (
                /[\w']/.test(codePart[j]) ||
                /[₀-₉]/.test(codePart[j]) ||
                codePart.charCodeAt(j) > 127
            )) j++;
            const w = codePart.slice(i, j), e = lrEsc(w);
            if (LR_KW.has(w)) out += `<span class="hl-keyword">${e}</span>`;
            else if (LR_TA.has(w)) out += `<span class="hl-tactic">${e}</span>`;
            else if (LR_TY.has(w)) out += `<span class="hl-type">${e}</span>`;
            else if (/^[A-Z]/.test(w)) out += `<span class="hl-type">${e}</span>`;
            else out += e;
            i = j; continue;
        }
        let hit = false;
        for (const op of ['^^^', '&&&', '|||', '<<<', '>>>', '<|>', ':=', '=>', '->', '<-', '::', '..']) {
            if (codePart.startsWith(op, i)) {
                out += `<span class="hl-op">${lrEsc(op)}</span>`;
                i += op.length; hit = true; break;
            }
        }
        if (hit) continue;
        out += lrEsc(ch); i++;
    }
    if (tailPart) out += `<span class="hl-comment">${lrEsc(tailPart)}</span>`;
    return out;
}

function hlLean(codeText) {
    return codeText.split('\n').map(lrHlLine).join('\n');
}

// ----------------------------------------------------------------
// Markdown + Math renderer
// Uses string-based placeholders so marked cannot strip them.
// This is the canonical implementation used by BOTH VSCode WebView
// and the HTML export. Do not duplicate this logic elsewhere.
// ----------------------------------------------------------------
function mdToHtml(content) {
    const mathBlocks = [];
    const PH_D = (i) => `LNMATH_D_${i}_END`;
    const PH_I = (i) => `LNMATH_I_${i}_END`;

    let s = content;
    // Strip leading --- line that marked would misinterpret as YAML front matter.
    // In Lean /-! blocks, --- is used as a horizontal rule / separator, not YAML.
    s = s.replace(/^\s*---\s*\n/, '\n');
    // Display math first (multi-line)
    s = s.replace(/\$\$([\s\S]*?)\$\$/g, (m, c) => {
        const i = mathBlocks.length;
        mathBlocks.push({ t: 'd', c });
        return PH_D(i);
    });
    // Inline math (single line, not crossing $)
    s = s.replace(/\$([^$\n]+?)\$/g, (m, c) => {
        const i = mathBlocks.length;
        mathBlocks.push({ t: 'i', c });
        return PH_I(i);
    });

    let html = (typeof marked !== 'undefined')
        ? marked.parse(s)
        : s.replace(/\n/g, '<br>');

    // Restore math with original delimiters.
    // IMPORTANT: use $$ and $ (not \[..\] / \(..\)) because the JS escape sequences
    // \[ and \( collapse to [ and ( in string literals, breaking MathJax recognition.
    html = html.replace(/LNMATH_D_(\d+)_END/g, (_, i) => `$$${mathBlocks[+i].c}$$`);
    html = html.replace(/LNMATH_I_(\d+)_END/g, (_, i) => `$${mathBlocks[+i].c}$`);
    return html;
}

// ----------------------------------------------------------------
// Mermaid renderer — single shared implementation
// Call renderMermaid(source, containerEl) from both main.js and template.html.
// ----------------------------------------------------------------
// (No CDN URL constants — all libraries loaded from local _libs/)
// ----------------------------------------------------------------

const MERMAID_THEME = {
    startOnLoad: false,
    theme: 'neutral',
    flowchart: { useMaxWidth: false },
    themeVariables: {
        background: '#ffffff',
        mainBkg: '#dbeafe',
        nodeBorder: '#93c5fd',
        lineColor: '#2563eb',
        textColor: '#1a2233',
        fontSize: '13px',
        primaryColor: '#dbeafe',
        primaryTextColor: '#1d4ed8',
        primaryBorderColor: '#93c5fd',
        edgeLabelBackground: '#f4f7fb',
    }
};

let _mermaidInitialized = false;
function ensureMermaidInit() {
    if (_mermaidInitialized) return;
    if (typeof mermaid === 'undefined') return;
    mermaid.initialize(MERMAID_THEME);
    _mermaidInitialized = true;
}

// Render a mermaid diagram into containerEl.
// Returns a Promise that resolves when done.
async function renderMermaid(source, containerEl) {
    if (typeof mermaid === 'undefined') {
        containerEl.textContent = 'Mermaid not loaded';
        return;
    }
    ensureMermaidInit();
    const id = 'mx-' + Math.random().toString(36).slice(2);
    try {
        const { svg } = await mermaid.render(id, source);
        containerEl.innerHTML = svg;
        // Remove Mermaid's inline height/max-height constraints.
        // Do NOT set width:100% — wide diagrams (e.g. dependency graphs)
        // would be forced into the container width, compressing height
        // proportionally via viewBox aspect-ratio preservation.
        // Instead, let the SVG keep its natural dimensions and rely on
        // CSS overflow-x:auto on .block-mermaid for horizontal scrolling.
        const svgEl = containerEl.querySelector('svg');
        if (svgEl) {
            svgEl.removeAttribute('height');
            svgEl.style.removeProperty('max-height');
        }
    } catch (e) {
        containerEl.textContent = `Mermaid Error: ${e.message}`;
    }
}

// ----------------------------------------------------------------
// Graphviz (DOT) renderer — via @viz-js/viz (Graphviz WASM)
// Single shared implementation for both WebView and HTML export.
// ----------------------------------------------------------------
let _vizInstance = null;
let _vizInstancePromise = null;

function getVizInstance() {
    if (_vizInstance) return Promise.resolve(_vizInstance);
    if (_vizInstancePromise) return _vizInstancePromise;
    _vizInstancePromise = new Promise((resolve, reject) => {
        // Viz.js may be loaded async; poll until it's available (up to 10s).
        let elapsed = 0;
        const interval = 100;
        const maxWait = 10000;
        function check() {
            if (typeof Viz !== 'undefined') {
                Viz.instance().then(viz => {
                    _vizInstance = viz;
                    resolve(viz);
                }).catch(reject);
            } else if (elapsed >= maxWait) {
                reject(new Error('Viz.js not loaded after ' + maxWait + 'ms'));
            } else {
                elapsed += interval;
                setTimeout(check, interval);
            }
        }
        check();
    });
    return _vizInstancePromise;
}

// Render a Graphviz DOT diagram into containerEl.
// Returns a Promise that resolves when done.
async function renderGraphviz(source, containerEl) {
    try {
        const viz = await getVizInstance();
        const svgEl = viz.renderSVGElement(source);
        containerEl.innerHTML = '';
        containerEl.appendChild(svgEl);
    } catch (e) {
        containerEl.textContent = `Graphviz Error: ${e.message || e}`;
    }
}

// ----------------------------------------------------------------
// MathJax: typeset a container and wrap display math.
// Call typesetMath(container) from BOTH main.js and template.html.
// This is the single shared implementation — do not duplicate.
// ----------------------------------------------------------------
function wrapDisplayMath(container) {
    container.querySelectorAll('mjx-container[display="true"]').forEach(el => {
        if (!el.parentElement.classList.contains('mjx-display-wrap')) {
            const wrap = document.createElement('div');
            wrap.className = 'mjx-display-wrap';
            el.parentNode.insertBefore(wrap, el);
            wrap.appendChild(el);
        }
    });
}

// Typeset MathJax in container, then wrap display math.
// Returns a Promise. Safe to call even if MathJax is not loaded.
function typesetMath(container) {
    if (window.MathJax && MathJax.typesetPromise) {
        return MathJax.typesetPromise([container])
            .then(() => wrapDisplayMath(container))
            .catch(console.warn);
    }
    return Promise.resolve();
}

// The canonical MathJax configuration object.
// Used verbatim in both NotebookPanel.ts (Extension) and template.html (HTML export).
const MATHJAX_CONFIG = {
    tex: {
        inlineMath: [['$', '$']],
        displayMath: [['$$', '$$']],
        processEscapes: true
    },
    options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
        menuOptions: {
            settings: {
                enrich: false,
                collapsible: false,
                speech: false,
                braille: false,
                assistiveMml: false
            }
        }
    },
    startup: { typeset: false }
};

// ----------------------------------------------------------------
// Lean comment parser (port of leanCommentParser.ts)
// ----------------------------------------------------------------
function trimEmptyLines(code) {
    const lines = code.split('\n');
    let s = 0;
    while (s < lines.length && lines[s].trim() === '') s++;
    let e = lines.length - 1;
    while (e >= 0 && lines[e].trim() === '') e--;
    if (s > e) return '';
    return lines.slice(s, e + 1).join('\n');
}

function dedent(str) {
    const lines = str.split('\n');
    let minIndent = Infinity;
    for (let i = 1; i < lines.length; i++) {
        const line = lines[i];
        if (line.trim().length === 0) continue;
        const m = line.match(/^\s*/);
        const indent = m ? m[0].length : 0;
        if (indent < minIndent) minIndent = indent;
    }
    if (minIndent === Infinity) minIndent = 0;
    return lines.map((line, idx) => {
        if (idx === 0) return line.trim();
        if (line.trim().length === 0) return '';
        return line.length >= minIndent ? line.slice(minIndent) : line.trim();
    }).join('\n').trim();
}

function findDocCommentEnd(text, startPos) {
    let pos = startPos;
    let inlineTickCount = null;
    let inFence = false;
    while (pos < text.length) {
        const ch = text[pos];
        if (ch === '`') {
            let run = 1;
            while (pos + run < text.length && text[pos + run] === '`') run++;
            if (inlineTickCount === null) {
                if (run >= 3) {
                    let i = pos - 1;
                    while (i >= 0 && text[i] !== '\n') i--;
                    const prefix = text.slice(i + 1, pos);
                    if (/^\s*$/.test(prefix)) { inFence = !inFence; pos += run; continue; }
                }
                if (!inFence) { inlineTickCount = run; pos += run; continue; }
            } else {
                if (run === inlineTickCount) { inlineTickCount = null; pos += run; continue; }
            }
            pos += run; continue;
        }
        const next = (pos + 1 < text.length) ? text[pos + 1] : '';
        if (!inFence && inlineTickCount === null && ch === '-' && next === '/') return pos;
        pos += 1;
    }
    return -1;
}

function splitLeanDocComments(text) {
    const blocks = [];
    let pos = 0, last = 0;

    function pushCode(code) {
        const lines = code.split('\n');
        let s = 0;
        while (s < lines.length && lines[s].trim() === '') s++;
        const rawStartLine = text.slice(0, last).split('\n').length - 1;
        const startLine = rawStartLine + s;
        const trimmedCode = trimEmptyLines(code);
        const trimmedLineCount = trimmedCode.split('\n').length;
        const endLine = startLine + (trimmedLineCount > 0 ? trimmedLineCount - 1 : 0);
        blocks.push({ type: 'code', source: trimmedCode, range: { startLine, endLine } });
    }

    function pushComment(kind, content, startOffset) {
        const dedentedContent = dedent(content);
        const startLine = text.slice(0, startOffset).split('\n').length - 1;
        const endLine = startLine + (content.split('\n').length - 1);
        blocks.push({ type: kind, content: dedentedContent, range: { startLine, endLine } });
    }

    while (pos < text.length) {
        const nextModule = text.indexOf('/-!', pos);
        const nextDoc = text.indexOf('/--', pos);
        let start = -1, kind = null;
        if (nextModule !== -1 && (nextDoc === -1 || nextModule < nextDoc)) { start = nextModule; kind = 'module-doc'; }
        else if (nextDoc !== -1) { start = nextDoc; kind = 'doc-comment'; }
        if (start === -1) break;
        if (start > last) pushCode(text.slice(last, start));
        const contentStart = start + 3;
        const end = findDocCommentEnd(text, contentStart);
        if (end === -1) { pushCode(text.slice(start)); last = text.length; break; }
        pushComment(kind, text.slice(contentStart, end), start);
        pos = end + 2; last = pos;
    }
    if (last < text.length) pushCode(text.slice(last));

    return blocks.filter(b => {
        if (b.type === 'code') return b.source.trim().length > 0;
        if (b.type === 'mermaid') return b.source.trim().length > 0;
        if (b.type === 'graphviz') return b.source.trim().length > 0;
        return b.content.trim().length > 0;
    });
}

function splitDiagramBlocks(content) {
    const result = [];
    // Note: backticks written as \x60 to avoid breaking HTML script-tag embedding
    const TICK3 = '\x60\x60\x60';
    const re = new RegExp('^' + TICK3 + '(mermaid|graphviz|dot)\\s*\\n([\\s\\S]*?)^' + TICK3 + '\\s*$', 'gm');
    let lastIndex = 0, match;
    while ((match = re.exec(content)) !== null) {
        const textContent = content.substring(lastIndex, match.index);
        if (textContent.trim().length > 0) result.push({ type: 'text', content: textContent.trim() });
        const lang = match[1]; // 'mermaid', 'graphviz', or 'dot'
        const src = match[2];
        if (src.trim().length > 0) {
            const blockType = lang === 'mermaid' ? 'mermaid' : 'graphviz';
            result.push({ type: blockType, source: trimEmptyLines(src) });
        }
        lastIndex = re.lastIndex;
    }
    if (lastIndex < content.length) {
        const remaining = content.substring(lastIndex);
        if (remaining.trim().length > 0) result.push({ type: 'text', content: remaining.trim() });
    }
    if (result.length === 0 && content.trim().length > 0)
        result.push({ type: 'text', content: content.trim() });
    return result;
}

function expandCommentBlock(block) {
    if (block.type !== 'module-doc' && block.type !== 'doc-comment') return [block];
    const subBlocks = splitDiagramBlocks(block.content);
    if (subBlocks.length === 1 && subBlocks[0].type === 'text') return [block];
    return subBlocks.map(sub =>
        sub.type === 'text'
            ? { type: block.type, content: sub.content, range: block.range }
            : { type: sub.type, source: sub.source, range: block.range }
    );
}

function parseLean(text) {
    return splitLeanDocComments(text).flatMap(b => expandCommentBlock(b));
}

  </script>
  <script>
      /* ================================================================
         Rendering and Boot — template.html specific UI logic
         All shared logic (hlLean, mdToHtml, parseLean etc.) lives in
         renderer.js which is inlined above by htmlExporter.ts at export time.
         ================================================================ */
      async function render(blocks) {
        const nb = document.getElementById('notebook');
        nb.innerHTML = '';

        for (const b of blocks) {
          if (b.type === 'module-doc' || b.type === 'doc-comment') {
            const cls = b.type === 'module-doc' ? 'block-module-doc' : 'block-doc-comment';
            const el = document.createElement('div');
            el.className = cls;
            el.innerHTML = mdToHtml(b.content);
            // Apply Lean syntax highlighting to ```lean code fences inside markdown
            el.querySelectorAll('pre code').forEach(code => {
              const isLean = code.classList.contains('language-lean') ||
                code.classList.contains('language-lean4');
              if (isLean) {
                code.innerHTML = hlLean(code.textContent || '');
              }
            });
            nb.appendChild(el);
          } else if (b.type === 'code') {
            const el = document.createElement('div');
            el.className = 'block-code';
            el.innerHTML = `<div class="block-code-header">lean4</div><pre class="lean-source">${hlLean(b.source)}</pre>`;
            nb.appendChild(el);
          } else if (b.type === 'mermaid') {
            const wrap = document.createElement('div');
            wrap.className = 'block-mermaid';
            nb.appendChild(wrap);
            await renderMermaid(b.source, wrap);
          } else if (b.type === 'graphviz') {
            const wrap = document.createElement('div');
            wrap.className = 'block-graphviz';
            nb.appendChild(wrap);
            await renderGraphviz(b.source, wrap);
          }
        }

        // TOC
        let tocHtml = '', hi = 0;
        nb.querySelectorAll('h1,h2,h3').forEach(h => {
          const id = 'h' + hi++; h.id = id;
          tocHtml += `<a href="#${id}" class="${h.tagName.toLowerCase()}">${h.textContent}</a>\n`;
        });
        document.getElementById('toc').innerHTML = tocHtml;

        const h1 = nb.querySelector('h1');
        if (h1) {
          document.getElementById('doc-title').textContent = h1.textContent;
          document.title = h1.textContent + ' — Lean Notebook';
        }

        // Use shared typesetMath() from renderer.js — MathJax
        typesetMath(nb);
      }

    /* ================================================================
       Boot
       ================================================================ */
    function boot() {
      if (typeof marked === 'undefined') { setTimeout(boot, 100); return; }
      marked.use({ gfm: true, breaks: true });
      const el = document.getElementById('lean-source');
      if (!el) return;

      const rawPre = document.getElementById('lean-raw-pre');
      rawPre.innerHTML = hlLean(el.textContent);

      const blocks = parseLean(el.textContent);
      render(blocks);

      const nb = document.getElementById('notebook');
      const leanRaw = document.getElementById('lean-raw');
      const appEl = document.getElementById('app');
      document.querySelectorAll('input[name="view"]').forEach(radio => {
        radio.addEventListener('change', () => {
          if (radio.value === 'lean') {
            nb.style.display = 'none';
            leanRaw.style.display = 'block';
            appEl.classList.add('lean-mode');
          } else {
            nb.style.display = '';
            leanRaw.style.display = 'none';
            appEl.classList.remove('lean-mode');
          }
        });
      });
    }
    window.addEventListener('load', boot);
  </script>
</body>

</html>